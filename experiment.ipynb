{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import cupy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from math import log10\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import join, isdir\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import cupy\n",
    "import re\n",
    "from torchvision.utils import save_image as imwrite\n",
    "import itertools\n",
    "import shutil\n",
    "\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_variable(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return (x) \n",
    "\n",
    "class KernelEstimation(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(KernelEstimation, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        def Basic(input_channel, output_channel):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=input_channel, out_channels=output_channel, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False),\n",
    "                torch.nn.Conv2d(in_channels=output_channel, out_channels=output_channel, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False),\n",
    "                torch.nn.Conv2d(in_channels=output_channel, out_channels=output_channel, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False)\n",
    "            )\n",
    "\n",
    "        def Upsample(channel):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                torch.nn.Conv2d(in_channels=channel, out_channels=channel, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False)\n",
    "            )\n",
    "\n",
    "        def Subnet(ks):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False),\n",
    "                torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False),\n",
    "                torch.nn.Conv2d(in_channels=64, out_channels=ks, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False),\n",
    "                torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                torch.nn.Conv2d(in_channels=ks, out_channels=ks, kernel_size=3, stride=1, padding=1)\n",
    "            )\n",
    "\n",
    "        self.moduleConv1 = Basic(6, 32)\n",
    "        self.modulePool1 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.moduleConv2 = Basic(32, 64)\n",
    "        self.modulePool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.moduleConv3 = Basic(64, 128)\n",
    "        self.modulePool3 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.moduleConv4 = Basic(128, 256)\n",
    "        self.modulePool4 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.moduleConv5 = Basic(256, 512)\n",
    "        self.modulePool5 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.moduleDeconv5 = Basic(512, 512)\n",
    "        self.moduleUpsample5 = Upsample(512)\n",
    "\n",
    "        self.moduleDeconv4 = Basic(512, 256)\n",
    "        self.moduleUpsample4 = Upsample(256)\n",
    "\n",
    "        self.moduleDeconv3 = Basic(256, 128)\n",
    "        self.moduleUpsample3 = Upsample(128)\n",
    "\n",
    "        self.moduleDeconv2 = Basic(128, 64)\n",
    "        self.moduleUpsample2 = Upsample(64)\n",
    "\n",
    "        self.moduleVertical1 = Subnet(self.kernel_size)\n",
    "        self.moduleVertical2 = Subnet(self.kernel_size)\n",
    "        self.moduleHorizontal1 = Subnet(self.kernel_size)\n",
    "        self.moduleHorizontal2 = Subnet(self.kernel_size)\n",
    "\n",
    "        # use this line only when testing on pretrained model. Comment it out when training\n",
    "        self.load_state_dict(torch.load(modelPath))\n",
    "\n",
    "    def forward(self, rfield0, rfield2):\n",
    "        tensorJoin = torch.cat([rfield0, rfield2], 1)\n",
    "\n",
    "        tensorConv1 = self.moduleConv1(tensorJoin)\n",
    "        tensorPool1 = self.modulePool1(tensorConv1)\n",
    "\n",
    "        tensorConv2 = self.moduleConv2(tensorPool1)\n",
    "        tensorPool2 = self.modulePool2(tensorConv2)\n",
    "\n",
    "        tensorConv3 = self.moduleConv3(tensorPool2)\n",
    "        tensorPool3 = self.modulePool3(tensorConv3)\n",
    "\n",
    "        tensorConv4 = self.moduleConv4(tensorPool3)\n",
    "        tensorPool4 = self.modulePool4(tensorConv4)\n",
    "\n",
    "        tensorConv5 = self.moduleConv5(tensorPool4)\n",
    "        tensorPool5 = self.modulePool5(tensorConv5)\n",
    "\n",
    "        tensorDeconv5 = self.moduleDeconv5(tensorPool5)\n",
    "        tensorUpsample5 = self.moduleUpsample5(tensorDeconv5)\n",
    "\n",
    "        tensorCombine = tensorUpsample5 + tensorConv5\n",
    "\n",
    "        tensorDeconv4 = self.moduleDeconv4(tensorCombine)\n",
    "        tensorUpsample4 = self.moduleUpsample4(tensorDeconv4)\n",
    "\n",
    "        tensorCombine = tensorUpsample4 + tensorConv4\n",
    "\n",
    "        tensorDeconv3 = self.moduleDeconv3(tensorCombine)\n",
    "        tensorUpsample3 = self.moduleUpsample3(tensorDeconv3)\n",
    "\n",
    "        tensorCombine = tensorUpsample3 + tensorConv3\n",
    "\n",
    "        tensorDeconv2 = self.moduleDeconv2(tensorCombine)\n",
    "        tensorUpsample2 = self.moduleUpsample2(tensorDeconv2)\n",
    "\n",
    "        tensorCombine = tensorUpsample2 + tensorConv2\n",
    "\n",
    "        Vertical1 = self.moduleVertical1(tensorCombine)\n",
    "        Vertical2 = self.moduleVertical2(tensorCombine)\n",
    "        Horizontal1 = self.moduleHorizontal1(tensorCombine)\n",
    "        Horizontal2 = self.moduleHorizontal2(tensorCombine)\n",
    "\n",
    "        return Vertical1, Horizontal1, Vertical2, Horizontal2\n",
    "\n",
    "kernel_Sepconv_updateOutput = '''\n",
    "    extern \"C\" __global__ void kernel_Sepconv_updateOutput(\n",
    "        const int n,\n",
    "        const float* input,\n",
    "        const float* vertical,\n",
    "        const float* horizontal,\n",
    "        float* output\n",
    "    ) { for (int intIndex = (blockIdx.x * blockDim.x) + threadIdx.x; intIndex < n; intIndex += blockDim.x * gridDim.x) {\n",
    "        float dblOutput = 0.0;\n",
    "\n",
    "        const int intSample = ( intIndex / SIZE_3(output) / SIZE_2(output) / SIZE_1(output) ) % SIZE_0(output);\n",
    "        const int intDepth  = ( intIndex / SIZE_3(output) / SIZE_2(output)                  ) % SIZE_1(output);\n",
    "        const int intY      = ( intIndex / SIZE_3(output)                                   ) % SIZE_2(output);\n",
    "        const int intX      = ( intIndex                                                    ) % SIZE_3(output);\n",
    "\n",
    "        for (int intFilterY = 0; intFilterY < SIZE_1(vertical); intFilterY += 1) {\n",
    "        for (int intFilterX = 0; intFilterX < SIZE_1(horizontal); intFilterX += 1) {\n",
    "        dblOutput += VALUE_4(input, intSample, intDepth, intY + intFilterY, intX + intFilterX) * VALUE_4(vertical, intSample, intFilterY, intY, intX) * VALUE_4(horizontal, intSample, intFilterX, intY, intX);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        output[intIndex] = dblOutput;\n",
    "    } }\n",
    "'''\n",
    "\n",
    "kernel_SeparableConvolution_updateGradVertical = '''\n",
    "    extern \"C\" __global__ void kernel_SeparableConvolution_updateGradVertical(\n",
    "        const int n,\n",
    "        const float* gradLoss,\n",
    "        const float* input,\n",
    "        const float* horizontal,\n",
    "        float* gradVertical\n",
    "    ) { for (int intIndex = (blockIdx.x * blockDim.x) + threadIdx.x; intIndex < n; intIndex += blockDim.x * gridDim.x) {\n",
    "        float floatOutput = 0.0;\n",
    "        float c = 0.0;\n",
    "    \n",
    "        const int intBatch   = ( intIndex / SIZE_3(gradVertical) / SIZE_2(gradVertical) / SIZE_1(gradVertical) ) % SIZE_0(gradVertical);\n",
    "        const int intFilterY = ( intIndex / SIZE_3(gradVertical) / SIZE_2(gradVertical)                        ) % SIZE_1(gradVertical);\n",
    "        const int intY       = ( intIndex / SIZE_3(gradVertical)                                               ) % SIZE_2(gradVertical);\n",
    "        const int intX       = ( intIndex                                                                      ) % SIZE_3(gradVertical);\n",
    "    \n",
    "        for (int intFilterX = 0; intFilterX < SIZE_1(horizontal); intFilterX++) \n",
    "        {\n",
    "            float product = VALUE_4(gradLoss, intBatch, 0, intY, intX)*              // channel 0\n",
    "            VALUE_4(input, intBatch, 0, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(horizontal, intBatch, intFilterX, intY, intX) +\n",
    "            VALUE_4(gradLoss, intBatch, 1, intY, intX)*                          // channel 1     \n",
    "            VALUE_4(input, intBatch, 1, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(horizontal, intBatch, intFilterX, intY, intX) +\n",
    "            VALUE_4(gradLoss, intBatch, 2, intY, intX)*                          // channel 2\n",
    "            VALUE_4(input, intBatch, 2, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(horizontal, intBatch, intFilterX, intY, intX);\n",
    "\n",
    "            floatOutput += product;\n",
    "        }\n",
    "    \n",
    "        gradVertical[intIndex] = floatOutput;\n",
    "    } }\n",
    "'''\n",
    "\n",
    "kernel_SeparableConvolution_updateGradHorizontal = '''\n",
    "    extern \"C\" __global__ void kernel_SeparableConvolution_updateGradHorizontal(\n",
    "        const int n,\n",
    "        const float* gradLoss,\n",
    "        const float* input,\n",
    "        const float* vertical,\n",
    "        float* gradHorizontal\n",
    "    ) { for (int intIndex = (blockIdx.x * blockDim.x) + threadIdx.x; intIndex < n; intIndex += blockDim.x * gridDim.x) {\n",
    "        float floatOutput = 0.0;\n",
    "        float c = 0.0;\n",
    "    \n",
    "        const int intBatch   = ( intIndex / SIZE_3(gradHorizontal) / SIZE_2(gradHorizontal) / SIZE_1(gradHorizontal) ) % SIZE_0(gradHorizontal);\n",
    "        const int intFilterX = ( intIndex / SIZE_3(gradHorizontal) / SIZE_2(gradHorizontal)                          ) % SIZE_1(gradHorizontal);\n",
    "        const int intY       = ( intIndex / SIZE_3(gradHorizontal)                                                   ) % SIZE_2(gradHorizontal);\n",
    "        const int intX       = ( intIndex                                                                            ) % SIZE_3(gradHorizontal);\n",
    "    \n",
    "        for (int intFilterY = 0; intFilterY < SIZE_1(vertical); intFilterY++)\n",
    "        {\n",
    "            float product = VALUE_4(gradLoss, intBatch, 0, intY, intX)*             // channel 0\n",
    "            VALUE_4(input, intBatch, 0, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(vertical, intBatch, intFilterY, intY, intX) + \n",
    "            VALUE_4(gradLoss, intBatch, 1, intY, intX)*                         // channel 1\n",
    "            VALUE_4(input, intBatch, 1, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(vertical, intBatch, intFilterY, intY, intX) + \n",
    "            VALUE_4(gradLoss, intBatch, 2, intY, intX)*                         // channel 2\n",
    "            VALUE_4(input, intBatch, 2, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(vertical, intBatch, intFilterY, intY, intX);\n",
    "    \n",
    "            float y = product - c;\n",
    "            float t = floatOutput + y;\n",
    "            c = (t - floatOutput) - y;\n",
    "            floatOutput = t;\n",
    "        }\n",
    "    \n",
    "        gradHorizontal[intIndex] = floatOutput;\n",
    "    } }\n",
    "'''\n",
    "\n",
    "def cupy_kernel(strFunction, objectVariables):\n",
    "    strKernel = globals()[strFunction]\n",
    "\n",
    "    while True:\n",
    "        objectMatch = re.search('(SIZE_)([0-4])(\\()([^\\)]*)(\\))', strKernel)\n",
    "\n",
    "        if objectMatch is None:\n",
    "            break\n",
    "        # end\n",
    "\n",
    "        intArg = int(objectMatch.group(2))\n",
    "\n",
    "        strTensor = objectMatch.group(4)\n",
    "        intSizes = objectVariables[strTensor].size()\n",
    "\n",
    "        strKernel = strKernel.replace(objectMatch.group(), str(intSizes[intArg]))\n",
    "    # end\n",
    "\n",
    "    while True:\n",
    "        objectMatch = re.search('(VALUE_)([0-4])(\\()([^\\)]+)(\\))', strKernel)\n",
    "\n",
    "        if objectMatch is None:\n",
    "            break\n",
    "        # end\n",
    "\n",
    "        intArgs = int(objectMatch.group(2))\n",
    "        strArgs = objectMatch.group(4).split(',')\n",
    "\n",
    "        strTensor = strArgs[0]\n",
    "        intStrides = objectVariables[strTensor].stride()\n",
    "        strIndex = ['((' + strArgs[intArg + 1].replace('{', '(').replace('}', ')').strip() + ')*' + str(intStrides[intArg]) + ')' for intArg in range(intArgs)]\n",
    "\n",
    "        strKernel = strKernel.replace(objectMatch.group(0), strTensor + '[' + str.join('+', strIndex) + ']')\n",
    "    # end\n",
    "\n",
    "    return strKernel\n",
    "\n",
    "\n",
    "@cupy.util.memoize(for_each_device=True)\n",
    "def cupy_launch(strFunction, strKernel):\n",
    "    return cupy.cuda.compile_with_cache(strKernel).get_function(strFunction)\n",
    "\n",
    "\n",
    "class FunctionSepconv(torch.autograd.Function):\n",
    "    def __init__(self):\n",
    "        super(FunctionSepconv, self).__init__()\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, input, vertical, horizontal):\n",
    "        self.save_for_backward(input, vertical, horizontal)\n",
    "\n",
    "        intSample = input.size(0)\n",
    "        intInputDepth = input.size(1)\n",
    "        intInputHeight = input.size(2)\n",
    "        intInputWidth = input.size(3)\n",
    "        intFilterSize = min(vertical.size(1), horizontal.size(1))\n",
    "        intOutputHeight = min(vertical.size(2), horizontal.size(2))\n",
    "        intOutputWidth = min(vertical.size(3), horizontal.size(3))\n",
    "\n",
    "        assert (intInputHeight - intFilterSize == intOutputHeight - 1)\n",
    "        assert (intInputWidth - intFilterSize == intOutputWidth - 1)\n",
    "\n",
    "        assert (input.is_contiguous() == True)\n",
    "        assert (vertical.is_contiguous() == True)\n",
    "        assert (horizontal.is_contiguous() == True)\n",
    "\n",
    "        output = input.new_zeros(intSample, intInputDepth, intOutputHeight, intOutputWidth)\n",
    "\n",
    "        if input.is_cuda == True:\n",
    "            class Stream:\n",
    "                ptr = torch.cuda.current_stream().cuda_stream\n",
    "\n",
    "            # end\n",
    "\n",
    "            n = output.nelement()\n",
    "            cupy_launch('kernel_Sepconv_updateOutput', cupy_kernel('kernel_Sepconv_updateOutput', {\n",
    "                'input': input,\n",
    "                'vertical': vertical,\n",
    "                'horizontal': horizontal,\n",
    "                'output': output\n",
    "            }))(\n",
    "                grid=tuple([int((n + 512 - 1) / 512), 1, 1]),\n",
    "                block=tuple([512, 1, 1]),\n",
    "                args=[n, input.data_ptr(), vertical.data_ptr(), horizontal.data_ptr(), output.data_ptr()],\n",
    "                stream=Stream\n",
    "            )\n",
    "\n",
    "        elif input.is_cuda == False:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # end\n",
    "\n",
    "        return output\n",
    "\n",
    "    # end\n",
    "\n",
    "    def backward(self, gradOutput):\n",
    "        input, vertical, horizontal = self.saved_tensors\n",
    "\n",
    "        intSample = input.size(0)\n",
    "        intInputDepth = input.size(1)\n",
    "        intInputHeight = input.size(2)\n",
    "        intInputWidth = input.size(3)\n",
    "        intFilterSize = min(vertical.size(1), horizontal.size(1))\n",
    "        intOutputHeight = min(vertical.size(2), horizontal.size(2))\n",
    "        intOutputWidth = min(vertical.size(3), horizontal.size(3))\n",
    "\n",
    "        assert (intInputHeight - intFilterSize == intOutputHeight - 1)\n",
    "        assert (intInputWidth - intFilterSize == intOutputWidth - 1)\n",
    "\n",
    "        assert (gradOutput.is_contiguous() == True)\n",
    "\n",
    "        gradInput = input.new_zeros(intSample, intInputDepth, intInputHeight, intInputWidth) if self.needs_input_grad[0] == True else None\n",
    "        gradVertical = input.new_zeros(intSample, intFilterSize, intOutputHeight, intOutputWidth) if self.needs_input_grad[1] == True else None\n",
    "        gradHorizontal = input.new_zeros(intSample, intFilterSize, intOutputHeight, intOutputWidth) if self.needs_input_grad[2] == True else None\n",
    "\n",
    "        if input.is_cuda == True:\n",
    "            class Stream:\n",
    "                ptr = torch.cuda.current_stream().cuda_stream\n",
    "\n",
    "            # end\n",
    "\n",
    "            # vertical grad\n",
    "            n_v = gradVertical.nelement()\n",
    "            cupy_launch('kernel_SeparableConvolution_updateGradVertical', cupy_kernel('kernel_SeparableConvolution_updateGradVertical', {\n",
    "                'gradLoss': gradOutput,\n",
    "                'input': input,\n",
    "                'horizontal': horizontal,\n",
    "                'gradVertical': gradVertical\n",
    "            }))(\n",
    "                grid=tuple([int((n_v + 512 - 1) / 512), 1, 1]),\n",
    "                block=tuple([512, 1, 1]),\n",
    "                args=[n_v, gradOutput.data_ptr(), input.data_ptr(), horizontal.data_ptr(), gradVertical.data_ptr()],\n",
    "                stream=Stream\n",
    "            )\n",
    "\n",
    "            # horizontal grad\n",
    "            n_h = gradHorizontal.nelement()\n",
    "            cupy_launch('kernel_SeparableConvolution_updateGradHorizontal', cupy_kernel('kernel_SeparableConvolution_updateGradHorizontal', {\n",
    "                'gradLoss': gradOutput,\n",
    "                'input': input,\n",
    "                'vertical': vertical,\n",
    "                'gradHorizontal': gradHorizontal\n",
    "            }))(\n",
    "                grid=tuple([int((n_h + 512 - 1) / 512), 1, 1]),\n",
    "                block=tuple([512, 1, 1]),\n",
    "                args=[n_h, gradOutput.data_ptr(), input.data_ptr(), vertical.data_ptr(), gradHorizontal.data_ptr()],\n",
    "                stream=Stream\n",
    "            )\n",
    "\n",
    "        elif input.is_cuda == False:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # end\n",
    "\n",
    "        return gradInput, gradVertical, gradHorizontal\n",
    "\n",
    "class SepConvNet(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(SepConvNet, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernel_pad = int(math.floor(kernel_size / 2.0))\n",
    "\n",
    "        self.epoch = Variable(torch.tensor(0, requires_grad=False))\n",
    "        self.get_kernel = KernelEstimation(self.kernel_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "        self.modulePad = torch.nn.ReplicationPad2d([self.kernel_pad, self.kernel_pad, self.kernel_pad, self.kernel_pad])\n",
    "\n",
    "    def forward(self, frame0, frame2):\n",
    "        h0 = int(list(frame0.size())[2]) # height of the 1st input image\n",
    "        w0 = int(list(frame0.size())[3]) # width of the 1st input image\n",
    "        h2 = int(list(frame2.size())[2]) # height of the 2nd input image\n",
    "        w2 = int(list(frame2.size())[3]) # width of the 2nd input image\n",
    "        if h0 != h2 or w0 != w2:\n",
    "            sys.exit('Frame sizes do not match')\n",
    "\n",
    "        h_padded = False\n",
    "        w_padded = False\n",
    "        if h0 % 32 != 0:\n",
    "            pad_h = 32 - (h0 % 32)\n",
    "            frame0 = F.pad(frame0, (0, 0, 0, pad_h))\n",
    "            frame2 = F.pad(frame2, (0, 0, 0, pad_h))\n",
    "            h_padded = True\n",
    "\n",
    "        if w0 % 32 != 0:\n",
    "            pad_w = 32 - (w0 % 32)\n",
    "            frame0 = F.pad(frame0, (0, pad_w, 0, 0))\n",
    "            frame2 = F.pad(frame2, (0, pad_w, 0, 0))\n",
    "            w_padded = True\n",
    "\n",
    "        Vertical1, Horizontal1, Vertical2, Horizontal2 = self.get_kernel(frame0, frame2)\n",
    "\n",
    "        tensorDot1 = FunctionSepconv()(self.modulePad(frame0), Vertical1, Horizontal1)\n",
    "        tensorDot2 = FunctionSepconv()(self.modulePad(frame2), Vertical2, Horizontal2)\n",
    "\n",
    "        frame1 = tensorDot1 + tensorDot2\n",
    "\n",
    "        if h_padded:\n",
    "            frame1 = frame1[:, :, 0:h0, :]\n",
    "        if w_padded:\n",
    "            frame1 = frame1[:, :, :, 0:w0]\n",
    "\n",
    "        return frame1\n",
    "\n",
    "    def train_model(self, frame0, frame2, frame1):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.forward(frame0, frame2)\n",
    "        loss = self.criterion(output, frame1)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def increase_epoch(self):\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DslfDataset(Dataset):\n",
    "    def __init__(self, input_dir):\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.doublet_list = np.array([os.path.join(input_dir, f) \n",
    "                                      for f in os.listdir(input_dir) \n",
    "                                      if os.path.isdir(os.path.join(input_dir, f)) \n",
    "                                      and f != '.ipynb_checkpoints'])\n",
    "    def __getitem__(self, index):\n",
    "        firstFrame = to_variable(self.transform(Image.open(os.path.join(self.doublet_list[index], \"first.png\"))))\n",
    "        secFrame = to_variable(self.transform(Image.open(os.path.join(self.doublet_list[index], \"sec.png\"))))\n",
    "        return firstFrame, secFrame\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.doublet_list)\n",
    "    \n",
    "    def doublet_list(self):\n",
    "        return self.doublet_list\n",
    "    \n",
    "    def stupid(self):\n",
    "        print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory in used: 0.210514944 Gb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testset = DslfDataset('my_input')\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size= 1, shuffle=False)\n",
    "print(f'Memory in used: {torch.cuda.memory_allocated()/1000000000} Gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory in used: 0.299191808 Gb\n",
      "0 torch.Size([1, 3, 720, 1280]) torch.Size([1, 3, 720, 1280])\n",
      "torch.Size([1, 3, 720, 1280])\n",
      "Time 0.6629748344421387\n",
      "Memory in used: 0.299191808 Gb\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "modelType = 'lf'\n",
    "modelPath = os.path.join(os.getcwd(), 'network-' + modelType + '.pytorch')\n",
    "model = SepConvNet(kernel_size=51).to('cuda')\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    for batch_idx, (firstFrame, secFrame) in enumerate(testloader):\n",
    "        print(f'Memory in used: {torch.cuda.memory_allocated()/1000000000} Gb')\n",
    "        print(batch_idx, firstFrame.shape, secFrame.shape)\n",
    "        frame_out = model(firstFrame, secFrame)\n",
    "        print(frame_out.shape)\n",
    "        for imageID in range(frame_out.shape[0]):\n",
    "            imwrite(frame_out[imageID], os.path.join('my_output', str(i) +'.png'), range=(0, 1))\n",
    "            i += 1\n",
    "    print('Time:', time.time() - start) \n",
    "       \n",
    "print(f'Memory in used: {torch.cuda.memory_allocated()/1000000000} Gb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Memory in used: {torch.cuda.memory_allocated()/1000000000} Gb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batch size 1: 151s\n",
    "* batch size 3: 152s\n",
    "* batch size 2: 149s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
