{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import cupy\n",
    "import math\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from math import log10\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import join, isdir\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import cupy\n",
    "import re\n",
    "from torchvision.utils import save_image as imwrite\n",
    "import itertools\n",
    "import shutil\n",
    "from torch.jit import script\n",
    "import time\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_variable(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return (x) \n",
    "\n",
    "class KernelEstimation(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(KernelEstimation, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        def Basic(input_channel, output_channel):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=input_channel, out_channels=output_channel, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False),\n",
    "                torch.nn.Conv2d(in_channels=output_channel, out_channels=output_channel, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False),\n",
    "                torch.nn.Conv2d(in_channels=output_channel, out_channels=output_channel, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False)\n",
    "            )\n",
    "\n",
    "        def Upsample(channel):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                torch.nn.Conv2d(in_channels=channel, out_channels=channel, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False)\n",
    "            )\n",
    "\n",
    "        def Subnet(ks):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False),\n",
    "                torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False),\n",
    "                torch.nn.Conv2d(in_channels=64, out_channels=ks, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.ReLU(inplace=False),\n",
    "                torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                torch.nn.Conv2d(in_channels=ks, out_channels=ks, kernel_size=3, stride=1, padding=1)\n",
    "            )\n",
    "\n",
    "        self.moduleConv1 = Basic(6, 32)\n",
    "        self.modulePool1 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.moduleConv2 = Basic(32, 64)\n",
    "        self.modulePool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.moduleConv3 = Basic(64, 128)\n",
    "        self.modulePool3 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.moduleConv4 = Basic(128, 256)\n",
    "        self.modulePool4 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.moduleConv5 = Basic(256, 512)\n",
    "        self.modulePool5 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.moduleDeconv5 = Basic(512, 512)\n",
    "        self.moduleUpsample5 = Upsample(512)\n",
    "\n",
    "        self.moduleDeconv4 = Basic(512, 256)\n",
    "        self.moduleUpsample4 = Upsample(256)\n",
    "\n",
    "        self.moduleDeconv3 = Basic(256, 128)\n",
    "        self.moduleUpsample3 = Upsample(128)\n",
    "\n",
    "        self.moduleDeconv2 = Basic(128, 64)\n",
    "        self.moduleUpsample2 = Upsample(64)\n",
    "\n",
    "        self.moduleVertical1 = Subnet(self.kernel_size)\n",
    "        self.moduleVertical2 = Subnet(self.kernel_size)\n",
    "        self.moduleHorizontal1 = Subnet(self.kernel_size)\n",
    "        self.moduleHorizontal2 = Subnet(self.kernel_size)\n",
    "\n",
    "        # use this line only when testing on pretrained model. Comment it out when training\n",
    "        self.load_state_dict(torch.load(modelPath))\n",
    "\n",
    "    def forward(self, rfield0, rfield2):\n",
    "        tensorJoin = torch.cat([rfield0, rfield2], 1)\n",
    "\n",
    "        tensorConv1 = self.moduleConv1(tensorJoin)\n",
    "        tensorPool1 = self.modulePool1(tensorConv1)\n",
    "\n",
    "        tensorConv2 = self.moduleConv2(tensorPool1)\n",
    "        tensorPool2 = self.modulePool2(tensorConv2)\n",
    "\n",
    "        tensorConv3 = self.moduleConv3(tensorPool2)\n",
    "        tensorPool3 = self.modulePool3(tensorConv3)\n",
    "\n",
    "        tensorConv4 = self.moduleConv4(tensorPool3)\n",
    "        tensorPool4 = self.modulePool4(tensorConv4)\n",
    "\n",
    "        tensorConv5 = self.moduleConv5(tensorPool4)\n",
    "        tensorPool5 = self.modulePool5(tensorConv5)\n",
    "\n",
    "        tensorDeconv5 = self.moduleDeconv5(tensorPool5)\n",
    "        tensorUpsample5 = self.moduleUpsample5(tensorDeconv5)\n",
    "\n",
    "        tensorCombine = tensorUpsample5 + tensorConv5\n",
    "\n",
    "        tensorDeconv4 = self.moduleDeconv4(tensorCombine)\n",
    "        tensorUpsample4 = self.moduleUpsample4(tensorDeconv4)\n",
    "\n",
    "        tensorCombine = tensorUpsample4 + tensorConv4\n",
    "\n",
    "        tensorDeconv3 = self.moduleDeconv3(tensorCombine)\n",
    "        tensorUpsample3 = self.moduleUpsample3(tensorDeconv3)\n",
    "\n",
    "        tensorCombine = tensorUpsample3 + tensorConv3\n",
    "\n",
    "        tensorDeconv2 = self.moduleDeconv2(tensorCombine)\n",
    "        tensorUpsample2 = self.moduleUpsample2(tensorDeconv2)\n",
    "\n",
    "        tensorCombine = tensorUpsample2 + tensorConv2\n",
    "\n",
    "        Vertical1 = self.moduleVertical1(tensorCombine)\n",
    "        Vertical2 = self.moduleVertical2(tensorCombine)\n",
    "        Horizontal1 = self.moduleHorizontal1(tensorCombine)\n",
    "        Horizontal2 = self.moduleHorizontal2(tensorCombine)\n",
    "\n",
    "        return Vertical1, Horizontal1, Vertical2, Horizontal2\n",
    "\n",
    "kernel_Sepconv_updateOutput = '''\n",
    "    extern \"C\" __global__ void kernel_Sepconv_updateOutput(\n",
    "        const int n,\n",
    "        const float* input,\n",
    "        const float* vertical,\n",
    "        const float* horizontal,\n",
    "        float* output\n",
    "    ) { for (int intIndex = (blockIdx.x * blockDim.x) + threadIdx.x; intIndex < n; intIndex += blockDim.x * gridDim.x) {\n",
    "        float dblOutput = 0.0;\n",
    "\n",
    "        const int intSample = ( intIndex / SIZE_3(output) / SIZE_2(output) / SIZE_1(output) ) % SIZE_0(output);\n",
    "        const int intDepth  = ( intIndex / SIZE_3(output) / SIZE_2(output)                  ) % SIZE_1(output);\n",
    "        const int intY      = ( intIndex / SIZE_3(output)                                   ) % SIZE_2(output);\n",
    "        const int intX      = ( intIndex                                                    ) % SIZE_3(output);\n",
    "\n",
    "        for (int intFilterY = 0; intFilterY < SIZE_1(vertical); intFilterY += 1) {\n",
    "        for (int intFilterX = 0; intFilterX < SIZE_1(horizontal); intFilterX += 1) {\n",
    "        dblOutput += VALUE_4(input, intSample, intDepth, intY + intFilterY, intX + intFilterX) * VALUE_4(vertical, intSample, intFilterY, intY, intX) * VALUE_4(horizontal, intSample, intFilterX, intY, intX);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        output[intIndex] = dblOutput;\n",
    "    } }\n",
    "'''\n",
    "\n",
    "kernel_SeparableConvolution_updateGradVertical = '''\n",
    "    extern \"C\" __global__ void kernel_SeparableConvolution_updateGradVertical(\n",
    "        const int n,\n",
    "        const float* gradLoss,\n",
    "        const float* input,\n",
    "        const float* horizontal,\n",
    "        float* gradVertical\n",
    "    ) { for (int intIndex = (blockIdx.x * blockDim.x) + threadIdx.x; intIndex < n; intIndex += blockDim.x * gridDim.x) {\n",
    "        float floatOutput = 0.0;\n",
    "        float c = 0.0;\n",
    "    \n",
    "        const int intBatch   = ( intIndex / SIZE_3(gradVertical) / SIZE_2(gradVertical) / SIZE_1(gradVertical) ) % SIZE_0(gradVertical);\n",
    "        const int intFilterY = ( intIndex / SIZE_3(gradVertical) / SIZE_2(gradVertical)                        ) % SIZE_1(gradVertical);\n",
    "        const int intY       = ( intIndex / SIZE_3(gradVertical)                                               ) % SIZE_2(gradVertical);\n",
    "        const int intX       = ( intIndex                                                                      ) % SIZE_3(gradVertical);\n",
    "    \n",
    "        for (int intFilterX = 0; intFilterX < SIZE_1(horizontal); intFilterX++) \n",
    "        {\n",
    "            float product = VALUE_4(gradLoss, intBatch, 0, intY, intX)*              // channel 0\n",
    "            VALUE_4(input, intBatch, 0, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(horizontal, intBatch, intFilterX, intY, intX) +\n",
    "            VALUE_4(gradLoss, intBatch, 1, intY, intX)*                          // channel 1     \n",
    "            VALUE_4(input, intBatch, 1, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(horizontal, intBatch, intFilterX, intY, intX) +\n",
    "            VALUE_4(gradLoss, intBatch, 2, intY, intX)*                          // channel 2\n",
    "            VALUE_4(input, intBatch, 2, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(horizontal, intBatch, intFilterX, intY, intX);\n",
    "\n",
    "            floatOutput += product;\n",
    "        }\n",
    "    \n",
    "        gradVertical[intIndex] = floatOutput;\n",
    "    } }\n",
    "'''\n",
    "\n",
    "kernel_SeparableConvolution_updateGradHorizontal = '''\n",
    "    extern \"C\" __global__ void kernel_SeparableConvolution_updateGradHorizontal(\n",
    "        const int n,\n",
    "        const float* gradLoss,\n",
    "        const float* input,\n",
    "        const float* vertical,\n",
    "        float* gradHorizontal\n",
    "    ) { for (int intIndex = (blockIdx.x * blockDim.x) + threadIdx.x; intIndex < n; intIndex += blockDim.x * gridDim.x) {\n",
    "        float floatOutput = 0.0;\n",
    "        float c = 0.0;\n",
    "    \n",
    "        const int intBatch   = ( intIndex / SIZE_3(gradHorizontal) / SIZE_2(gradHorizontal) / SIZE_1(gradHorizontal) ) % SIZE_0(gradHorizontal);\n",
    "        const int intFilterX = ( intIndex / SIZE_3(gradHorizontal) / SIZE_2(gradHorizontal)                          ) % SIZE_1(gradHorizontal);\n",
    "        const int intY       = ( intIndex / SIZE_3(gradHorizontal)                                                   ) % SIZE_2(gradHorizontal);\n",
    "        const int intX       = ( intIndex                                                                            ) % SIZE_3(gradHorizontal);\n",
    "    \n",
    "        for (int intFilterY = 0; intFilterY < SIZE_1(vertical); intFilterY++)\n",
    "        {\n",
    "            float product = VALUE_4(gradLoss, intBatch, 0, intY, intX)*             // channel 0\n",
    "            VALUE_4(input, intBatch, 0, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(vertical, intBatch, intFilterY, intY, intX) + \n",
    "            VALUE_4(gradLoss, intBatch, 1, intY, intX)*                         // channel 1\n",
    "            VALUE_4(input, intBatch, 1, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(vertical, intBatch, intFilterY, intY, intX) + \n",
    "            VALUE_4(gradLoss, intBatch, 2, intY, intX)*                         // channel 2\n",
    "            VALUE_4(input, intBatch, 2, intY + intFilterY, intX + intFilterX)* \n",
    "            VALUE_4(vertical, intBatch, intFilterY, intY, intX);\n",
    "    \n",
    "            float y = product - c;\n",
    "            float t = floatOutput + y;\n",
    "            c = (t - floatOutput) - y;\n",
    "            floatOutput = t;\n",
    "        }\n",
    "    \n",
    "        gradHorizontal[intIndex] = floatOutput;\n",
    "    } }\n",
    "'''\n",
    "\n",
    "def cupy_kernel(strFunction, objectVariables):\n",
    "    strKernel = globals()[strFunction]\n",
    "\n",
    "    while True:\n",
    "        objectMatch = re.search('(SIZE_)([0-4])(\\()([^\\)]*)(\\))', strKernel)\n",
    "\n",
    "        if objectMatch is None:\n",
    "            break\n",
    "        # end\n",
    "\n",
    "        intArg = int(objectMatch.group(2))\n",
    "\n",
    "        strTensor = objectMatch.group(4)\n",
    "        intSizes = objectVariables[strTensor].size()\n",
    "\n",
    "        strKernel = strKernel.replace(objectMatch.group(), str(intSizes[intArg]))\n",
    "    # end\n",
    "\n",
    "    while True:\n",
    "        objectMatch = re.search('(VALUE_)([0-4])(\\()([^\\)]+)(\\))', strKernel)\n",
    "\n",
    "        if objectMatch is None:\n",
    "            break\n",
    "        # end\n",
    "\n",
    "        intArgs = int(objectMatch.group(2))\n",
    "        strArgs = objectMatch.group(4).split(',')\n",
    "\n",
    "        strTensor = strArgs[0]\n",
    "        intStrides = objectVariables[strTensor].stride()\n",
    "        strIndex = ['((' + strArgs[intArg + 1].replace('{', '(').replace('}', ')').strip() + ')*' + str(intStrides[intArg]) + ')' for intArg in range(intArgs)]\n",
    "\n",
    "        strKernel = strKernel.replace(objectMatch.group(0), strTensor + '[' + str.join('+', strIndex) + ']')\n",
    "    # end\n",
    "\n",
    "    return strKernel\n",
    "\n",
    "\n",
    "@cupy.util.memoize(for_each_device=True)\n",
    "def cupy_launch(strFunction, strKernel):\n",
    "    return cupy.cuda.compile_with_cache(strKernel).get_function(strFunction)\n",
    "\n",
    "\n",
    "class FunctionSepconv(torch.autograd.Function):\n",
    "    def __init__(self):\n",
    "        super(FunctionSepconv, self).__init__()\n",
    "\n",
    "    # end\n",
    "    def forward(self, input, vertical, horizontal):\n",
    "        self.save_for_backward(input, vertical, horizontal)\n",
    "\n",
    "        intSample = input.size(0)\n",
    "        intInputDepth = input.size(1)\n",
    "        intInputHeight = input.size(2)\n",
    "        intInputWidth = input.size(3)\n",
    "        intFilterSize = min(vertical.size(1), horizontal.size(1))\n",
    "        intOutputHeight = min(vertical.size(2), horizontal.size(2))\n",
    "        intOutputWidth = min(vertical.size(3), horizontal.size(3))\n",
    "\n",
    "        assert (intInputHeight - intFilterSize == intOutputHeight - 1)\n",
    "        assert (intInputWidth - intFilterSize == intOutputWidth - 1)\n",
    "\n",
    "        assert (input.is_contiguous() == True)\n",
    "        assert (vertical.is_contiguous() == True)\n",
    "        assert (horizontal.is_contiguous() == True)\n",
    "\n",
    "        output = input.new_zeros(intSample, intInputDepth, intOutputHeight, intOutputWidth)\n",
    "\n",
    "        if input.is_cuda == True:\n",
    "            class Stream:\n",
    "                ptr = torch.cuda.current_stream().cuda_stream\n",
    "\n",
    "            # end\n",
    "\n",
    "            n = output.nelement()\n",
    "            cupy_launch('kernel_Sepconv_updateOutput', cupy_kernel('kernel_Sepconv_updateOutput', {\n",
    "                'input': input,\n",
    "                'vertical': vertical,\n",
    "                'horizontal': horizontal,\n",
    "                'output': output\n",
    "            }))(\n",
    "                grid=tuple([int((n + 512 - 1) / 512), 1, 1]),\n",
    "                block=tuple([512, 1, 1]),\n",
    "                args=[n, input.data_ptr(), vertical.data_ptr(), horizontal.data_ptr(), output.data_ptr()],\n",
    "                stream=Stream\n",
    "            )\n",
    "\n",
    "        elif input.is_cuda == False:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # end\n",
    "\n",
    "        return output\n",
    "\n",
    "    # end\n",
    "\n",
    "    def backward(self, gradOutput):\n",
    "        input, vertical, horizontal = self.saved_tensors\n",
    "\n",
    "        intSample = input.size(0)\n",
    "        intInputDepth = input.size(1)\n",
    "        intInputHeight = input.size(2)\n",
    "        intInputWidth = input.size(3)\n",
    "        intFilterSize = min(vertical.size(1), horizontal.size(1))\n",
    "        intOutputHeight = min(vertical.size(2), horizontal.size(2))\n",
    "        intOutputWidth = min(vertical.size(3), horizontal.size(3))\n",
    "\n",
    "        assert (intInputHeight - intFilterSize == intOutputHeight - 1)\n",
    "        assert (intInputWidth - intFilterSize == intOutputWidth - 1)\n",
    "\n",
    "        assert (gradOutput.is_contiguous() == True)\n",
    "\n",
    "        gradInput = input.new_zeros(intSample, intInputDepth, intInputHeight, intInputWidth) if self.needs_input_grad[0] == True else None\n",
    "        gradVertical = input.new_zeros(intSample, intFilterSize, intOutputHeight, intOutputWidth) if self.needs_input_grad[1] == True else None\n",
    "        gradHorizontal = input.new_zeros(intSample, intFilterSize, intOutputHeight, intOutputWidth) if self.needs_input_grad[2] == True else None\n",
    "\n",
    "        if input.is_cuda == True:\n",
    "            class Stream:\n",
    "                ptr = torch.cuda.current_stream().cuda_stream\n",
    "\n",
    "            # end\n",
    "\n",
    "            # vertical grad\n",
    "            n_v = gradVertical.nelement()\n",
    "            cupy_launch('kernel_SeparableConvolution_updateGradVertical', cupy_kernel('kernel_SeparableConvolution_updateGradVertical', {\n",
    "                'gradLoss': gradOutput,\n",
    "                'input': input,\n",
    "                'horizontal': horizontal,\n",
    "                'gradVertical': gradVertical\n",
    "            }))(\n",
    "                grid=tuple([int((n_v + 512 - 1) / 512), 1, 1]),\n",
    "                block=tuple([512, 1, 1]),\n",
    "                args=[n_v, gradOutput.data_ptr(), input.data_ptr(), horizontal.data_ptr(), gradVertical.data_ptr()],\n",
    "                stream=Stream\n",
    "            )\n",
    "\n",
    "            # horizontal grad\n",
    "            n_h = gradHorizontal.nelement()\n",
    "            cupy_launch('kernel_SeparableConvolution_updateGradHorizontal', cupy_kernel('kernel_SeparableConvolution_updateGradHorizontal', {\n",
    "                'gradLoss': gradOutput,\n",
    "                'input': input,\n",
    "                'vertical': vertical,\n",
    "                'gradHorizontal': gradHorizontal\n",
    "            }))(\n",
    "                grid=tuple([int((n_h + 512 - 1) / 512), 1, 1]),\n",
    "                block=tuple([512, 1, 1]),\n",
    "                args=[n_h, gradOutput.data_ptr(), input.data_ptr(), vertical.data_ptr(), gradHorizontal.data_ptr()],\n",
    "                stream=Stream\n",
    "            )\n",
    "\n",
    "        elif input.is_cuda == False:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # end\n",
    "\n",
    "        return gradInput, gradVertical, gradHorizontal\n",
    "\n",
    "class SepConvNet(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(SepConvNet, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernel_pad = int(math.floor(kernel_size / 2.0))\n",
    "\n",
    "        self.epoch = Variable(torch.tensor(0, requires_grad=False))\n",
    "        self.get_kernel = KernelEstimation(self.kernel_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "        self.modulePad = torch.nn.ReplicationPad2d([self.kernel_pad, self.kernel_pad, self.kernel_pad, self.kernel_pad])\n",
    "\n",
    "    def forward(self, frame0, frame2):\n",
    "        h0 = int(list(frame0.size())[2]) # height of the 1st input image\n",
    "        w0 = int(list(frame0.size())[3]) # width of the 1st input image\n",
    "        h2 = int(list(frame2.size())[2]) # height of the 2nd input image\n",
    "        w2 = int(list(frame2.size())[3]) # width of the 2nd input image\n",
    "        if h0 != h2 or w0 != w2:\n",
    "            sys.exit('Frame sizes do not match')\n",
    "\n",
    "        h_padded = False\n",
    "        w_padded = False\n",
    "        if h0 % 32 != 0:\n",
    "            pad_h = 32 - (h0 % 32)\n",
    "            frame0 = F.pad(frame0, (0, 0, 0, pad_h))\n",
    "            frame2 = F.pad(frame2, (0, 0, 0, pad_h))\n",
    "            h_padded = True\n",
    "\n",
    "        if w0 % 32 != 0:\n",
    "            pad_w = 32 - (w0 % 32)\n",
    "            frame0 = F.pad(frame0, (0, pad_w, 0, 0))\n",
    "            frame2 = F.pad(frame2, (0, pad_w, 0, 0))\n",
    "            w_padded = True\n",
    "\n",
    "        Vertical1, Horizontal1, Vertical2, Horizontal2 = self.get_kernel(frame0, frame2)\n",
    "\n",
    "        tensorDot1 = FunctionSepconv()(self.modulePad(frame0), Vertical1, Horizontal1)\n",
    "        tensorDot2 = FunctionSepconv()(self.modulePad(frame2), Vertical2, Horizontal2)\n",
    "\n",
    "        frame1 = tensorDot1 + tensorDot2\n",
    "\n",
    "        if h_padded:\n",
    "            frame1 = frame1[:, :, 0:h0, :]\n",
    "        if w_padded:\n",
    "            frame1 = frame1[:, :, :, 0:w0]\n",
    "\n",
    "        return frame1\n",
    "\n",
    "    def train_model(self, frame0, frame2, frame1):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.forward(frame0, frame2)\n",
    "        loss = self.criterion(output, frame1)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def increase_epoch(self):\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DslfDataset(Dataset):\n",
    "    def __init__(self, input_dir):\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.doublet_list = np.array([os.path.join(input_dir, f) \n",
    "                                      for f in os.listdir(input_dir) \n",
    "                                      if os.path.isdir(os.path.join(input_dir, f)) \n",
    "                                      and f != '.ipynb_checkpoints'])\n",
    "    def __getitem__(self, index):\n",
    "        firstFrame = to_variable(self.transform(Image.open(os.path.join(self.doublet_list[index], \"first.png\"))))\n",
    "        secFrame = to_variable(self.transform(Image.open(os.path.join(self.doublet_list[index], \"sec.png\"))))\n",
    "        return firstFrame, secFrame\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.doublet_list)\n",
    "    \n",
    "    def doublet_list(self):\n",
    "        return self.doublet_list\n",
    "    \n",
    "    def stupid(self):\n",
    "        print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory in used: 0.0 Gb\n"
     ]
    }
   ],
   "source": [
    "testset = DslfDataset('my_input')\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size= 1, shuffle=False)\n",
    "print(f'Memory in used: {torch.cuda.memory_allocated()/1000000000} Gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory in used: 0.299191808 Gb\n",
      "0 torch.Size([1, 3, 720, 1280]) torch.Size([1, 3, 720, 1280])\n",
      "torch.Size([1, 3, 720, 1280])\n",
      "Time 0.6629748344421387\n",
      "Memory in used: 0.299191808 Gb\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "modelType = 'lf'\n",
    "modelPath = os.path.join(os.getcwd(), 'network-' + modelType + '.pytorch')\n",
    "model = SepConvNet(kernel_size=51).to('cuda')\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    for batch_idx, (firstFrame, secFrame) in enumerate(testloader):\n",
    "        print(f'Memory in used: {torch.cuda.memory_allocated()/1000000000} Gb')\n",
    "        print(batch_idx, firstFrame.shape, secFrame.shape)\n",
    "        frame_out = model(firstFrame, secFrame)\n",
    "        print(frame_out.shape)\n",
    "        for imageID in range(frame_out.shape[0]):\n",
    "            imwrite(frame_out[imageID], os.path.join('my_output', str(i) +'.png'), range=(0, 1))\n",
    "            i += 1\n",
    "    print('Time:', time.time() - start) \n",
    "       \n",
    "print(f'Memory in used: {torch.cuda.memory_allocated()/1000000000} Gb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Memory in used: {torch.cuda.memory_allocated()/1000000000} Gb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batch size 1: 151s\n",
    "* batch size 3: 152s\n",
    "* batch size 2: 149s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PyTorch Production\n",
    "How to convert your models into a new representation called Torch Script. Using two methods, tracing and annotations, you can convert your models into ScriptModules which can be exported as serialized files then loaded into a C++ application.  \n",
    "Using this combination of `Torch Script` and the `PyTorch C++ API`, you can do all the *development and training of your network in Python*, then *utilize the trained model in a C++ application.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 TRACING MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The idea behind `Tracing` is that you have your model and example data, then you pass your example data through your model (the forward pass). Behind the scene, what PyTorch is doing is keeping track of all the operations that are being performed on your input tensor, this way it can build the graph of operations that are being performed on your input. When it has the graph, it can convert the model to `Torch Script`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `torch.jit`, a just-in-time (JIT) compiler that at runtime takes your PyTorch models and rewrites them to run at production-efficiency. The JIT compiler can also export your model to run in a C++-only runtime based on Caffe2 bits.  \n",
    " The PyTorch tracer, `torch.jit.trace`, is a function that records all the native PyTorch operations performed in a code region, along with the data dependencies between them. In fact, PyTorch has had a tracer since 0.3, which has been used for exporting models through ONNX. What changes now, is that you no longer necessarily need to take the trace and run it elsewhere - PyTorch can re-execute it for you, using a carefully designed high-performance C++ runtime. As we develop PyTorch 1.0 this runtime will integrate all the optimizations and hardware integrations that Caffe2 provides.  \n",
    " The biggest benefit of this approach is that it doesn’t really care how your Python code is structured — you can trace through generators or coroutines, modules or pure functions. Since we only record native PyTorch operators, these details have no effect on the trace recorded. This behavior, however, is a double-edged sword. For example, if you have a loop in your model, it will get unrolled in the trace, inserting a copy of the loop body for as many times as the loop ran. This opens up opportunities for zero-cost abstraction (e.g. you can loop over modules, and the actual trace will be loop-overhead free!), but on the other hand this will also affect data dependent loops (think of e.g. processing sequences of varying lengths), effectively hard-coding a single length into the trace.\n",
    "\n",
    "For networks that do not contain loops and if statements, tracing is non-invasive and is robust enough to handle a wide variety of coding styles. This code example illustrates what tracing looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguyen29\\AppData\\Local\\Continuum\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\ipykernel_launcher.py:390: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "C:\\Users\\nguyen29\\AppData\\Local\\Continuum\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\ipykernel_launcher.py:391: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "C:\\Users\\nguyen29\\AppData\\Local\\Continuum\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\ipykernel_launcher.py:392: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "C:\\Users\\nguyen29\\AppData\\Local\\Continuum\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\ipykernel_launcher.py:393: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to trace FunctionSepconv, but tracing of legacy functions is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3ec9f5fe6725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdataIter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mfirstFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecFrame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataIter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# return the next item of an iterable object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtraced_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfirstFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# the Torch Script representation of the model, but can be used as the regular model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# The training loop doesn't change. Traced model behaves exactly like an\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\jit\\__init__.py\u001b[0m in \u001b[0;36mtrace\u001b[1;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class)\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[0mtraced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_module_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mexecutor_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         traced._c._create_method_from_trace('forward', func, example_inputs,\n\u001b[1;32m--> 688\u001b[1;33m                                             var_lookup_fn, _force_outplace)\n\u001b[0m\u001b[0;32m    689\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__name__'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'forward'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-bca47d461bf0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, frame0, frame2)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0mVertical1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHorizontal1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVertical2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHorizontal2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[0mtensorDot1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionSepconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodulePad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVertical1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHorizontal1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m         \u001b[0mtensorDot2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionSepconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodulePad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVertical2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHorizontal2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to trace FunctionSepconv, but tracing of legacy functions is not supported"
     ]
    }
   ],
   "source": [
    "# This will run your nn.Module or regular Python function with the example\n",
    "# input that you provided. The returned callable can be used to re-execute\n",
    "# all operations that happened during the example run, but it will no longer\n",
    "# use the Python interpreter.\n",
    "from torch.jit import trace\n",
    "# get the model\n",
    "modelType = 'lf'\n",
    "modelPath = os.path.join(os.getcwd(), 'network-' + modelType + '.pytorch')\n",
    "model = SepConvNet(kernel_size=51).to('cuda')\n",
    "# get the example input\n",
    "dataIter = iter(testloader)\n",
    "firstFrame, secFrame = dataIter.next() # return the next item of an iterable object \n",
    "traced_model = trace(model, (firstFrame, secFrame)) # the Torch Script representation of the model, but can be used as the regular model\n",
    "\n",
    "# The training loop doesn't change. Traced model behaves exactly like an\n",
    "# nn.Module, except that you can't edit what it does or change its attributes.\n",
    "# Think of it as a \"frozen module\".\n",
    "# for input, target in data_loader:\n",
    "#     loss = loss_fn(traced_model(input), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 SCRIPT MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing mode is a great way to minimize the impact on your code, but we’re also very excited about the models that fundamentally make use of control flow such as RNNs. Our solution to this is a scripting mode.\n",
    "\n",
    "In this case you write out a regular Python function, except that you can no longer use certain more complicated language features. Once you isolated the desired functionality, you let us know that you’d like the function to get compiled by decorating it with an `@script` decorator. This annotation will transform your python function directly into our high-performance C++ runtime. This lets us recover all the PyTorch operations along with loops and conditionals. They will be embedded into our internal representation of this function, and will be accounted for every time this function is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.jit import script\n",
    "\n",
    "@script\n",
    "def rnn_loop(x):\n",
    "    hidden = None\n",
    "    for x_t in x.split(1):\n",
    "        x, hidden = model(x, hidden)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of whether you use `tracing` or `@script`, the result is a python-free representation of your model, which can be used to optimize the model or to export the model from python for use in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distributed Programming with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(rank, size):\n",
    "    print('run')\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "def init_processes(rank, size, fn, backend='tcp'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    print('init')\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    run(rank, size)\n",
    "\n",
    "def main():\n",
    "    size = 2\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_processes, args=(rank, size, run))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        print(processes)\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Process(Process-11, started)>]\n",
      "[<Process(Process-11, started)>, <Process(Process-12, started)>]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" All-Reduce example.\"\"\"\n",
    "def run(rank, size):\n",
    "    \"\"\" Simple point-to-point communication. \"\"\"\n",
    "    group = dist.new_group([0, 1])\n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.distributed' has no attribute 'new_group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c6d0d7bc16cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-a57a1d65a88e>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(rank, size)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"\"\" Simple point-to-point communication. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.distributed' has no attribute 'new_group'"
     ]
    }
   ],
   "source": [
    "run(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dataset partitioning helper \"\"\"\n",
    "class Partition(object):\n",
    "\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPartitioner(object):\n",
    "\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Partitioning MNIST \"\"\"\n",
    "def partition_dataset():\n",
    "    dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "    size = dist.get_world_size()\n",
    "    bsz = 128 / float(size)\n",
    "    partition_sizes = [1.0 / size for _ in range(size)]\n",
    "    partition = DataPartitioner(dataset, partition_sizes)\n",
    "    partition = partition.use(dist.get_rank())\n",
    "    train_set = torch.utils.data.DataLoader(partition,\n",
    "                                         batch_size=bsz,\n",
    "                                         shuffle=True)\n",
    "    return train_set, bsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Distributed Synchronous SGD Example \"\"\"\n",
    "def run(rank, size):\n",
    "    torch.manual_seed(1234)\n",
    "    train_set, bsz = partition_dataset()\n",
    "    model = Net()\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=0.01, momentum=0.5)\n",
    "\n",
    "    num_batches = ceil(len(train_set.dataset) / float(bsz))\n",
    "    for epoch in range(10):\n",
    "        epoch_loss = 0.0\n",
    "        for data, target in train_set:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            average_gradients(model)\n",
    "            optimizer.step()\n",
    "        print('Rank ', dist.get_rank(), ', epoch ',\n",
    "              epoch, ': ', epoch_loss / num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Gradient averaging. \"\"\"\n",
    "def average_gradients(model):\n",
    "    size = float(dist.get_world_size())\n",
    "    for param in model.parameters():\n",
    "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
    "        param.grad.data /= size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implementation of a ring-reduce with addition. \"\"\"\n",
    "def allreduce(send, recv):\n",
    "    rank = dist.get_rank()\n",
    "    size = dist.get_world_size()\n",
    "    send_buff = th.zeros(send.size())\n",
    "    recv_buff = th.zeros(send.size())\n",
    "    accum = th.zeros(send.size())\n",
    "    accum[:] = send[:]\n",
    "\n",
    "    left = ((rank - 1) + size) % size\n",
    "    right = (rank + 1) % size\n",
    "\n",
    "    for i in range(size - 1):\n",
    "        if i % 2 == 0:\n",
    "            # Send send_buff\n",
    "            send_req = dist.isend(send_buff, right)\n",
    "            dist.recv(recv_buff, left)\n",
    "            accum[:] += recv[:]\n",
    "        else:\n",
    "            # Send recv_buff\n",
    "            send_req = dist.isend(recv_buff, right)\n",
    "            dist.recv(send_buff, left)\n",
    "            accum[:] += send[:]\n",
    "        send_req.wait()\n",
    "    recv[:] = accum[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.multiprocessing import Pool, Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size for training and testing\n",
    "batch_size = 32\n",
    "\n",
    "# Number of additional worker processes for dataloading\n",
    "workers = 2\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 2\n",
    "\n",
    "# Starting Learning Rate\n",
    "starting_lr = 0.1\n",
    "\n",
    "# Number of distributed processes\n",
    "world_size = 4\n",
    "\n",
    "# Distributed backend type\n",
    "dist_backend = 'nccl'\n",
    "\n",
    "# Url used to setup distributed training\n",
    "dist_url = \"tcp://172.31.22.234:23456\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Process Group...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.distributed' has no attribute 'init_process_group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-a3a37243216f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Initialize Process Group\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# v1 - init with url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_process_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdist_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdist_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# v2 - init with file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# dist.init_process_group(backend=\"nccl\", init_method=\"file:///home/ubuntu/pt-distributed-tutorial/trainfile\", rank=int(sys.argv[1]), world_size=world_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.distributed' has no attribute 'init_process_group'"
     ]
    }
   ],
   "source": [
    "print(\"Initialize Process Group...\")\n",
    "# Initialize Process Group\n",
    "# v1 - init with url\n",
    "dist.init_process_group(backend=dist_backend, init_method=dist_url, rank=int(sys.argv[1]), world_size=world_size)\n",
    "# v2 - init with file\n",
    "# dist.init_process_group(backend=\"nccl\", init_method=\"file:///home/ubuntu/pt-distributed-tutorial/trainfile\", rank=int(sys.argv[1]), world_size=world_size)\n",
    "# v3 - init with environment variables\n",
    "# dist.init_process_group(backend=\"nccl\", init_method=\"env://\", rank=int(sys.argv[1]), world_size=world_size)\n",
    "\n",
    "\n",
    "# Establish Local Rank and set device on this node\n",
    "local_rank = int(sys.argv[2])\n",
    "dp_device_ids = [local_rank]\n",
    "torch.cuda.set_device(local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributed.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Image.open('./my_input/0/castle.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transform(Image.open('./my_input/0/castle.png')).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 720, 1280])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = transform(Image.open('./my_input/0/first.bmp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1200, 1920])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004975318908691406\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "bs = b.repeat(3, 1, 1)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1200, 1920])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bss = bs.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1200, 1920])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03399467468261719\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c = transform(Image.open('./my_input/0/first.bmp').convert('RGB'))\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1200, 1920])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
